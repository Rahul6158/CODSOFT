{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNOu2ArEmkxJ9eake42ZITl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahul6158/CODSOFT/blob/main/Task_1_Movie_Genre_Classification(LR).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Movie_Genre_Classification Using Logistic Regression"
      ],
      "metadata": {
        "id": "fTFA4vZrQOOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tLB67TNIWEP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re  # for pattern matching and text manipulation.\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"train_data.txt\", sep=':::', names=[\"title\", \"genre\", \"description\"], engine='python')\n",
        "test_data = pd.read_csv(\"test_data.txt\", sep=':::', names=[\"title\", \"description\"], engine='python')"
      ],
      "metadata": {
        "id": "P1euuoNaI-Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "YCkqq7ESJgog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "ShtJ-ieLJktl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30,10))\n",
        "counts = train_data.genre.value_counts()\n",
        "sns.barplot(x=counts.index, y=counts)\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90);"
      ],
      "metadata": {
        "id": "ah8ux94WKUpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "UXHmcHZ4KUs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.genre.value_counts()"
      ],
      "metadata": {
        "id": "Z58mmWUlK1Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "ag-IokTJK1hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "vSO59S20K1mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape before drop nulls\",train_data.shape)\n",
        "# Droping the redundant data\n",
        "train_data = train_data.drop_duplicates()\n",
        "print(\"shape after drop nulls\",train_data.shape)"
      ],
      "metadata": {
        "id": "USuyjJ2jK1qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "AiQPeJHINq6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) # stop words\n",
        "def clean_data(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@\\S+','',text)                       # [1] remove mentions\n",
        "    text = re.sub(r'http\\S+', '', text)                  # [2] remove urls\n",
        "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+',\"\",text)          # [3] remove emails\n",
        "    text = re.sub(r\"[^a-zA-Z+']\", ' ', text)             # [4] keep only english chars / remove numbers\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text+' ')      # [5] remove single chars\n",
        "    text = re.sub(r'pic.\\S+', '',text)                   # [6]\n",
        "    text = re.sub(r'#', \"\", text)                        # [7] remove hashtags\n",
        "    text = re.sub(r\"_\", \"  \", text)                      # [8] remove hashtags\n",
        "    text = re.sub('\\n',\" . \",text)                       # [9] remove new lines\n",
        "    text = re.sub('\\[[^]]*\\]','',text)                   # [10] remove square prackets\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation]) # [11] remove punctuations\n",
        "    text= re.sub(\"\\s[\\s]+\", \" \",text).strip()            # [12] remove repeated/leading/trailing spaces\n",
        "    tokens = word_tokenize(text)                         # [13] Tokenize\n",
        "    text = \" \".join([word for word in tokens if word not in stop_words and len(word) > 2]) # [14] remove stop words\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "8vcT_az9K_1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['description_cleaned'] = train_data['description'].apply(clean_data)\n",
        "test_data['description_cleaned'] = test_data['description'].apply(clean_data)"
      ],
      "metadata": {
        "id": "3p0xEcYyK_4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st = PorterStemmer()\n",
        "train_data['description_cleaned'] = train_data['description_cleaned'].apply(\n",
        "    lambda x: ' '.join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "test_data['description_cleaned'] = test_data['description_cleaned'].apply(\n",
        "    lambda x: ' '.join([st.stem(word) for word in x.split()]))"
      ],
      "metadata": {
        "id": "0Ds5FhCjK_7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['length']=train_data['description'].apply(len)\n",
        "train_data['length_cleaned']=train_data['description_cleaned'].apply(len)\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "sUd8JKsiLAAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average Length of Text Before Cleaning: \", train_data['length'].mean())\n",
        "print(\"Average Length of Text After Cleaning: \", train_data['length_cleaned'].mean())"
      ],
      "metadata": {
        "id": "P8fZfiSALADj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 7))\n",
        "\n",
        "sns.histplot(data=train_data, x='length', bins=20, kde=True, color='blue')\n",
        "\n",
        "plt.xlabel('Length', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
        "plt.title('Distribution of Lengths', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eHjfAdsHLcPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_bigram(corpus, n=None):\n",
        "    vec = CV(ngram_range=(2, 2)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "common_words = get_top_n_bigram(train_data['description_cleaned'], 10)\n",
        "common_words_df = pd.DataFrame(common_words,columns=['word','freq'])\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x='freq', y='word', data=common_words_df,facecolor='yellow',linewidth=3,edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",10))\n",
        "\n",
        "plt.title(\"Top 10 bigrams\",font='Serif')\n",
        "plt.xlabel(\"Frequency\", fontsize=10)\n",
        "plt.yticks(fontsize=13)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.ylabel(\"\");"
      ],
      "metadata": {
        "id": "ROiqxZDOLcSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "train_data['genre'] = le.fit_transform(train_data['genre'].values)\n",
        "\n",
        "# keep only relevent columns\n",
        "train_df = train_data.loc[:,['description_cleaned', 'genre']]\n",
        "test_df = test_data.loc[:,['description_cleaned', 'title']]\n",
        "train_df.head(10)"
      ],
      "metadata": {
        "id": "9AjUvSHhLcVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set , val_set , train_label , val_label = train_test_split(train_df['description_cleaned'] , train_data['genre'] , test_size=0.2 , shuffle=True , random_state = 42)\n",
        "\n",
        "print(f'Split data into train and eval sets')\n",
        "print(f'Trani Set\\t: {len(train_set)}\\nValidation Set\\t: {len(val_set)}')"
      ],
      "metadata": {
        "id": "-QHOhRyLLcXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using TF-IDF\n",
        "vectorize = TfidfVectorizer(stop_words='english', max_features=100000)\n",
        "train_set_tfidf = vectorize.fit_transform(train_set)\n",
        "val_set_tfidf = vectorize.transform(val_set)"
      ],
      "metadata": {
        "id": "gzMW1tYrLcZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "n80S5g7rPIml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR_model = LogisticRegression()\n",
        "LR_model.fit(train_set_tfidf, train_label)\n",
        "predict_LR = LR_model.predict(val_set_tfidf)\n",
        "print(classification_report(val_label, predict_LR))\n",
        "LR_accuracy = accuracy_score(predict_LR,val_label)\n",
        "print('Logistic Regression accuracy is: {:.2f}%'.format(LR_accuracy*100))"
      ],
      "metadata": {
        "id": "65CP9byBL0vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Naive Bayes classifier\n",
        "NB_model = MultinomialNB()\n",
        "NB_model.fit(train_set_tfidf, train_label)\n",
        "y_pred_naive = NB_model.predict(val_set_tfidf)\n",
        "print(classification_report(val_label, y_pred_naive))\n",
        "naive_accuracy = accuracy_score(y_pred_naive,val_label)\n",
        "print('Naive Bayes model accuracy is: {:.2f}%'.format(naive_accuracy*100))"
      ],
      "metadata": {
        "id": "IHrd6vZcL0yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DT = DecisionTreeClassifier(max_depth=(1), random_state=0)\n",
        "DT.fit(train_set_tfidf, train_label)\n",
        "predict_ID3 = DT.predict(val_set_tfidf)\n",
        "print(classification_report(val_label, predict_ID3))\n",
        "ID3_accuracy = accuracy_score(predict_ID3,val_label)\n",
        "print('ID3 model accuracy is: {:.2f}%'.format(ID3_accuracy*100))"
      ],
      "metadata": {
        "id": "w6NuQ0EQL01M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a SVC classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "svm_model = LinearSVC()\n",
        "svm_model.fit(train_set_tfidf, train_label)\n",
        "predict = svm_model.predict(val_set_tfidf)\n",
        "\n",
        "print(classification_report(val_label, predict))\n",
        "svm_accuracy = accuracy_score(predict,val_label)\n",
        "print('SVC model accuracy is: {:.2f}%'.format(svm_accuracy*100))"
      ],
      "metadata": {
        "id": "y58eyG9KL03s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns=['LogisticRegression', 'MultinomialNB', 'Decision_Tree','SVC']\n",
        "accuracy= [LR_accuracy, naive_accuracy, ID3_accuracy, svm_accuracy]\n",
        "\n",
        "FinalResult=pd.DataFrame({'Algorithm':columns, 'Accuracy':accuracy})\n",
        "\n",
        "FinalResult"
      ],
      "metadata": {
        "id": "rdRwJ4Q0L06Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(figsize=(15,5))\n",
        "plt.plot(FinalResult.Algorithm,accuracy,label=\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cfA7AdHgL08i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}